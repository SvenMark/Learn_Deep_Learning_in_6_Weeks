{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.3.1'"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "import glob # The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "import imageio # Imageio is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, volumetric data, and scientific formats.\n",
    "# Magic function - sets the output of plotting commands to inline so that the output is below the jupiter cell\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow.keras.layers as layers # Keras layers API\n",
    "import time\n",
    "from IPython import display # For displaying image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array split does not result in an equal division",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001B[0m in \u001B[0;36msplit\u001B[1;34m(ary, indices_or_sections, axis)\u001B[0m\n\u001B[0;32m    864\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 865\u001B[1;33m         \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindices_or_sections\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    866\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'int' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-349-276daa9647bf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;31m# shuffle and batch the data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_images\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m \u001B[0mtrain_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_images\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mBUFFER_SIZE\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36msplit\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001B[0m in \u001B[0;36msplit\u001B[1;34m(ary, indices_or_sections, axis)\u001B[0m\n\u001B[0;32m    869\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mN\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0msections\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    870\u001B[0m             raise ValueError(\n\u001B[1;32m--> 871\u001B[1;33m                 'array split does not result in an equal division')\n\u001B[0m\u001B[0;32m    872\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0marray_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mary\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices_or_sections\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    873\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: array split does not result in an equal division"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import PIL\n",
    "\n",
    "DATA_DIR = \"../datasets/processed-celeba-small/processed_celeba_small/\"\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "\n",
    "def load_image( infilename ) :\n",
    "    img = PIL.Image.open( infilename )\n",
    "    img = img.crop([25,65,153,193])\n",
    "    img = img.resize((64,64))\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data\n",
    "\n",
    "train_images = np.array(os.listdir(DATA_DIR))\n",
    "np.random.shuffle(train_images)\n",
    "BUFFER_SIZE = 100#200000 # number of images in training i think\n",
    "BATCH_SIZE = 5#500 # This is just the standard number for batch size. Google for more info\n",
    "# shuffle and batch the data\n",
    "np.random.shuffle(train_images)\n",
    "train_images = np.split(train_images[:BUFFER_SIZE],BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(4*4*1024, use_bias=False, input_shape=(100,)))\n",
    "\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((4, 4, 1024)))\n",
    "    assert model.output_shape == (None, 4, 4, 1024)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "    assert model.output_shape == (None, 8, 8, 512)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Another transposed convolutional layer (upsampling)\n",
    "    model.add(layers.Conv2DTranspose(256, (5,5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "    assert model.output_shape == (None, 16, 16, 256)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Another transposed convolutional layer (upsampling)\n",
    "    model.add(layers.Conv2DTranspose(128, (5,5), strides = (2,2), padding = \"same\", use_bias = False))\n",
    "    assert model.output_shape == (None, 32, 32, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Final output layer also a convolutional layer (upsampling), sigmoid goes from 0 to 1\n",
    "    model.add(layers.Conv2DTranspose(3, (5,5), strides = (2,2), padding = \"same\", use_bias = False, activation = \"sigmoid\"))\n",
    "    assert model.output_shape == (None, 64, 64, 3)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x18e19ffca58>"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0], interpolation='nearest')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[64,64,3]))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(1))\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-2.0487519e-06]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i])\n",
    "      plt.axis('off')\n",
    "\n",
    "  if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "  plt.savefig('images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 288x288 with 16 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOwAAADnCAYAAAAdFLrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADGUlEQVR4nO3asY3EMAwAQfFx/bfM7+CgzNjzTCoFYrAgYHh29wANf08/ALgnWAgRLIQIFkIECyGfb4czk/6EvLtzc+8tc57znll/dU4bFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAiZ3X36DcAlGxZCBAshgoUQwUKIYCFEsBAiWAgRLIR8vh3OTPqvit2dm3tvmfOc98z6q3PasBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoWQ2d2n3wBcsmEhRLAQIlgIESyECBZCBAshgoUQwUKIYCHk8+1wZtK/Qe3u3Nx7y5znvGfWX53ThoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyEzO4+/Qbgkg0LIYKFEMFCiGAhRLAQIlgIESyECBZCBAshn2+HM5P+DWp35+beW+Y85z2z/uqcNiyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhRLAQIlgIESyECBZCBAshgoUQwUKIYCFEsBAiWAgRLIQIFkIECyGChRDBQohgIUSwECJYCBEshAgWQgQLIYKFEMFCiGAhZHb36TcAl2xYCBEshAgWQgQLIYKFEMFCyD8OjFDD5WvHdQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 50.4012086391449 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-348-d152560ca122>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mEPOCHS\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-346-802af7bf198a>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(dataset, epochs)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mimage_batch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m       \u001B[0mtrain_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;31m# Produce images for the GIF as we go\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    805\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    806\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 807\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    808\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    809\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1846\u001B[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001B[0;32m   1847\u001B[0m         \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1848\u001B[1;33m         cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1850\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1923\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1924\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1926\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    548\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mc:\\users\\sven mark\\documents\\git\\deeplearning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}