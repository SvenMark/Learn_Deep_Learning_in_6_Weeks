{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(28, 28, 1)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.8),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.8),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1)\n",
    "], name='discriminator')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "  tf.keras.Input(shape=(latent_dim,)),\n",
    "    # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "    layers.Dense(7 * 7 * 128),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "    layers.Reshape((7, 7, 128)),\n",
    "    layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "    layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "    layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"tanh\"),\n",
    "], name='generator')\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override the train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "  def __init__(self, discriminator, generator, latent_dim):\n",
    "    super(GAN, self).__init__()\n",
    "    self.discriminator = discriminator\n",
    "    self.generator = generator\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "  def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "    super(GAN, self).compile()\n",
    "    self.d_optimizer = d_optimizer\n",
    "    self.g_optimizer = g_optimizer\n",
    "    self.loss_fn = loss_fn\n",
    "\n",
    "  def train_step(self, real_images):\n",
    "    if isinstance(real_images, tuple):\n",
    "      real_images = real_images[0]\n",
    "    # Sample random points in the latent space\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "    # Decode them to fake images\n",
    "    generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "    # Combine them with real images\n",
    "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "    # Assemble labels discriminating real from fake images\n",
    "    labels = tf.concat([\n",
    "      tf.ones((batch_size, 1)),\n",
    "      tf.zeros((batch_size, 1))\n",
    "    ], axis=0)\n",
    "\n",
    "    # Add random noise to the labels - important trick!\n",
    "    labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "\n",
    "    # Train the discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = self.discriminator(combined_images)\n",
    "      d_loss = self.loss_fn(labels, predictions)\n",
    "    grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "    self.d_optimizer.apply_gradients(\n",
    "        zip(grads, self.discriminator.trainable_weights)\n",
    "    )\n",
    "\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "    # Assemble labels that say \"all real images\"\n",
    "    misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the generator (note that we should *not* update the weights\n",
    "    # of the discriminator)!\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "        g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "    grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "    self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "    return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a callback save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, num_img=3, latent_dim=128):\n",
    "    self.num_img = num_img\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "    generated_images = self.model.generator(random_latent_vectors)\n",
    "    generated_images = generated_images * 127.5 + 127.5\n",
    "    generated_images.numpy()\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    for i in range(self.num_img):\n",
    "        img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    if not os.path.exists('images'):\n",
    "      os.makedirs('images')\n",
    "    plt.savefig('images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a callback save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "\n",
    "gan.load_weights(checkpoint_path)\n",
    "\n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    train_ds, epochs=epochs, callbacks=[GANMonitor(num_img=16, latent_dim=latent_dim), cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}